{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Solving QAP\"\n",
    "author: \"Łukasz Andryszewski 151930\"\n",
    "date: \"May 2025\"\n",
    "subtitle: \"Using a heuristic, local search, random algorithms, simulated annealing and tabu search.\"\n",
    "titlepage: true\n",
    "lof: true\n",
    "lot: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "# Problem and solver description\n",
    "\n",
    "## Qaudratic Assignment Problem (QAP)\n",
    "\n",
    "The tackled problem is NP-hard, meaning there's no polynomial time algorithm that guarantees optimum. The goal of the optimization is:\n",
    "\n",
    "$$ \\min_{\\pi} \\sum_{i=0}^{n}\\sum_{j=0}^{n} a_{ij} \\cdot b_{\\pi_{i}\\pi_{j}}$$\n",
    "\n",
    "The problem can be thought of as assigning some 'departments' to some 'locations'. The $a$ is the flow of communication between the 'departments' and $b$ is a distance matrix between the 'locations'. The $\\pi$ is a permutation, which describes to which 'locations', different 'departments' are assigned. The aim is to minimise the cost of communication between all of the 'departments', by controlling their 'location'.\n",
    "\n",
    "The problem has real world applications, like in the case of the $\\texttt{KraXXx}$ instances. These contain real world data, which was used to plan the Klinikum Regensburg in Germany. Or the $\\texttt{EscXXx}$ instances which \"...stem from an application in computer science, from the testing of self-testable sequential\n",
    "circuits\".\n",
    "\n",
    "### Chosen instances:\n",
    "\n",
    "Instances bigger in size were selected, to limit test the solver. For each type, as described in the QAPLIB description, it was desirable to have it in multiple sizes in order to potentially analyze the impact of the size of instance. It was also important for the instance to have the optimum known.\n",
    "\n",
    "- $\\texttt{Lipa20a}$, $\\texttt{Lipa50a}$ and $\\texttt{Lipa90a}$ examples were chosen because they are generated using the same method, the optimum is known and they are assymetric which makes them somewhat interesting.\n",
    "\n",
    "- $\\texttt{Esc16a}$, $\\texttt{Esc32g}$ and $\\texttt{Esc128}$, because they come from real life problems and are of different sizes.\n",
    "\n",
    "- $\\texttt{tai50b}$, $\\texttt{tai100b}$ and $\\texttt{tai150b}$, because they are all assymetric and generated the same way\n",
    "\n",
    "- $\\texttt{tai64c}$, $\\texttt{tai256c}$ because they 'occur in the generation of grey patterns' and also contain the largest instance\n",
    "\n",
    "## Solver implementation\n",
    "\n",
    "The solver used to analyse the problem was written in C++ and compiled using g++. It can appropriately handle assymetric instances. It implements:\n",
    "\n",
    "- Local search (in steepest and greedy version) $\\text{---}$ local_search\n",
    "- Random walk $\\text{---}$ random_walk\n",
    "- Random search $\\text{---}$ random_seach\n",
    "- a construction Heuristic $\\text{---}$ heuristic\n",
    "- Tabu search $\\text{---}$ tabu_search\n",
    "- Simulated Annealing $\\text{---}$ simulated_annealing\n",
    "\n",
    "Construction heuristic calculates value of adding a new assignment at the end. Then the best assignment is kept. An additional boolean array is used to determine if a 'location' is already assigned. Additionally the first assignment is random to make the heuristic non-deterministic.\n",
    "\n",
    "The program is used as follows:\n",
    "\n",
    "```bat\n",
    "bio_solver.exe (instance:str) (solver_name:std) (repetitions:int) (solver_args...)\n",
    "```\n",
    "\n",
    "Local search takes ```steepest``` or ```greedy``` as an argument. Random walk and random search take ```duration``` in nanoseconds as an argument.\n",
    "\n",
    "The program outputs:\n",
    "\n",
    "- instance_name instance_size optimal_value\n",
    "- optimal_solution\n",
    "- number_of_repetitions\n",
    "Then repetitions of:\n",
    "- initial_solution\n",
    "- final_solution\n",
    "- starting_value final_value\n",
    "- evaluations steps execution_time\n",
    "\n",
    "```run_all.bat``` program takes instances as an argument and then runs all the solvers on these instances, with predefined number of repetitions. The execution time for RS and RW is taken as an average of execution times of greedy and steepest local search.\n",
    "\n",
    "### Simulated Annealing\n",
    "\n",
    "Simulated annealing arguments:\n",
    "\n",
    "- number of cycles $P$ without improvements $\\text{---}$ which is then multiplied by Markov chain length $L$ $\\text{---}$ the algorithm stops after $P\\cdot L$ steps without improvment.\n",
    "- initial acceptance $\\text{---}$ percentage of neighbours to be accepted by the initial temperature\n",
    "- temperature decrease $\\text{---}$ ratio of how much the temperature should decrease after a given chain $(\\alpha)$ $\\text{---}$ $c(k+1) = c(k) \\cdot \\alpha$\n",
    "- chain lenght ratio $\\text{---}$ percentage of neighbourhood size, which determines the size of Markov chains\n",
    "\n",
    "The algorithm is inspired by the natural process of annealing, in which a material is slowly cooled to achieve a crystalic structure. In theory, with the right parameters and with a globally convex fitness landscape the algorithm can find the optimal value. \n",
    "\n",
    "In this implementation the algorithm:\n",
    "1. Selects initial temperature, by scanning the deltas randomly sampled from the solution space. Then the average acceptance is calculated and using the Secant method the algorithm finds the temperature that satisfies the desired acceptance. Secant method is an algorithm for finding the root of a finding using numerical-analysis. It is recurrent and uses the following formula:\n",
    "$$ x_n = x_{n-1} - f(x_{n-1}) \\cdot \\frac{x_{n-1} - x_{n-2}}{f(x_{n-1}) - f(x_{n-2})}$$ \n",
    "The starting $x_0$ and $x_1$ are selected to be $10^{-3}$ and $1$ respectively to avoid the method exploding. The algorithm is still able to find solutions beside that range. To find the temperature, the average acceptance is calculated and from it the desired acceptance is subtracted.\n",
    "\n",
    "2. For each length of the markov chain ($L$) the temperature is the same. Then a next move in the neighbourhood is picked and applied if accepted. If a move is applied the neighbourhood is shuffled. After each evaluation the stopping condition is verified.\n",
    "\n",
    "3. After the chain ends, the temperature $c$ is lowered as specified earlier:\n",
    "\n",
    "$$c(k+1) = c(k) \\cdot \\alpha $$ \n",
    "3. When after $P \\cdot L$ iterations(here not equivalent to a \"step\", but just the evaluation) with no improvement the current chain ends. Temperature is then lowered to $10^{-4}$ and run until no improvement is triggered again. If the temperature already is that low, the algorithm ends.\n",
    "\n",
    "### Tabu Search\n",
    "\n",
    "Tabu search arguments:\n",
    "\n",
    "- number of steps without improvement \n",
    "- tabu tenure $\\text{---}$ number of steps when a move is tabu, given as a percentage of size of instance\n",
    "- top percent $\\text{---}$ used to calculate the $k$ best solutions to consider in elite candidate list strategy\n",
    "- max quality drop $\\text{---}$ how much the quality of a move can drop in relation to the current solution value for the candidate list to be reconstructed\n",
    "\n",
    "The algorithm tries to introduce exploration by accepting non improving neighbours. To avoid cycles a tabu list is kept, which marks a move *tabu* for *tabu tenure* iterations.\n",
    "\n",
    "The procedure applies two aspiration criteria:\n",
    "\n",
    "- aspiration by default $\\text{---}$ if all moves are tabu, accept the least tabu move.\n",
    "\n",
    "- aspiration by optimization objective $\\text{---}$ accept a move, even if it is tabu, when applying it would bring get objective value, better than the best seen.\n",
    "\n",
    "In this implementation an elite candidate list is introduced, to lower the computation cost.\n",
    "\n",
    "1. At the start of the iteration, all solutions in the neighbourhood are evaluated and sorted.\n",
    "\n",
    "2. Then the best $k$ moves, become the elite candidates. In each iteration the best value move $m_b$ is found and the best value move that is also the least tabu $m_t$.\n",
    "\n",
    "3. If $m_b$ would produce a better individual than known to this point, the move is applied even if is tabu, which suffices the global aspiration criteria. Otherwise if $m_b$ the $m_t$ is applied, which satisfies aspiration by default.\n",
    "\n",
    "\n",
    "In both algorithms if the value a solution is better then the best and the solution is about to be degraded $(\\delta > 0)$, the current solution is saved as the best known. This way if the solution improves multiple times in a row, its not copied each time, but only when its about to get worse (like doing a checkpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhood\n",
    "\n",
    "The defining operator in local search is how the neighbourhood of a solution is defined. Local search implemented hear used the 2-OPT neighbourhood. For this operator the size can be defined as:\n",
    "\n",
    "$$ N = \\dfrac{n^2 - n}{2} $$\n",
    "\n",
    "Which can be thought of as the upper triangular part of a square matrix. The neighbourhood is initialized, for each instance of local search, by creating every combination of two positions. Then its shuffled. A random offset is initialized to help with the randomization of ordering, whilst not spending much time on additional reshuffling. This is especially important for greedy, because the first found improvement is selected in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import pandify\n",
    "from itertools import product\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_over(start_val, final_val, opt):\n",
    "    # assuming minimization of objective\n",
    "    # to be minimized\n",
    "    return (start_val - final_val) / opt\n",
    "\n",
    "def quality_to_opt(final_val, opt):\n",
    "    return quality_over(final_val,opt,opt)\n",
    "\n",
    "def similarity(sol1, sol2):\n",
    "    return np.mean(sol1==sol2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\"lipa20a\",\"lipa50a\",\"lipa90a\",\"esc16a\",\"esc32g\",\"esc128\",\"tai50b\",\"tai100b\", \"tai150b\",\"tai64c\",\"tai256c\"]\n",
    "#print(*instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instances = list(\"esc128 tai256c wil100 tho150 lipa90b chr25a bur26a rou12\".split())\n",
    "solvers = [\"ls_greedy\", \"ls_steepest\", \"rs\", \"rw\", \"heuristic\", \"tabu_search\", \"simulated_annealing\"]\n",
    "shorter_solvers = [\"G\",\"S\",\"RS\",\"RW\", \"H\", \"TS\", \"SA\"]\n",
    "full_solvers = [\"local_search_greedy\",\"local_search_steepest\",\"random_search\",\"random_walk\", \"heuristic\", \"tabu_search\", \"simulated_annealing\"]\n",
    "solver_map = {short:full for short,full in zip(solvers,full_solvers)}\n",
    "solver_to_short_map = {full:short for short,full in zip(shorter_solvers,full_solvers)}\n",
    "colors = [\"seagreen\", \"mediumblue\", \"red\", \"orange\", \"hotpink\", \"turquoise\", \"darkviolet\"]\n",
    "color_map = {solver:color for solver,color in zip(full_solvers, colors)}\n",
    "img = \"img\"\n",
    "img_size = (8.3,11.7)\n",
    "img_format = \".pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandify(instances,solvers,solver_map)\n",
    "#data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters selected\n",
    "\n",
    "Advanced metaheuristic algorithms (TS and SA) were run 50 times on each instance. The rest of the methods were run 300 times. The figures however only show 50 runs for each.\n",
    "\n",
    "### Simulated Annealing\n",
    "\n",
    "The chosen parameters were:\n",
    "- P = 20\n",
    "- initial acceptance = 0.95\n",
    "- temperature decrease = 0.9\n",
    "- ratio of chain length = 2.0 (of the neighbourhood size)\n",
    "\n",
    "The temperature was chosen to decrease slowly to incentivise exploration. For that the P and ratio of chain length were increased so that the algorithm does not end prematurely. The chain length can be over $1.0$, because the neighbourhood is shuffled and the solution changes when the move is accepted.\n",
    "\n",
    "The values were fitted, so the algorithm achieves at least as good results as local searches and performs more evaluations.\n",
    "\n",
    "### Tabu Search\n",
    "\n",
    "The chosen parameters were:\n",
    "- no improvement iterations = 250\n",
    "- tabu tenure = 0.25 (of the instance size)\n",
    "- top percent = 0.2 (of the instance size, which calculates to k)\n",
    "- max quality drop = 0.1\n",
    "\n",
    "The tabu search was tuned to perform in as similar timeframe as possible, to simulated annealing. They were also tuned so that they achieve at least as good results as local searches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time was measured in nanoseconds with the C++ ```chrono``` library using a ```high_resolution_clock```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Running times of algorithms](img/run_times.pdf){#fig:run_times}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flierprops = dict(marker='o', markerfacecolor='black', markersize=5,markeredgecolor='none',alpha=0.2)\n",
    "X = data.pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")[\"time\"]\n",
    "fig, axs = plt.subplots(ncols=3,nrows=4,layout=\"tight\",sharex=True,sharey='row')\n",
    "fig.set_size_inches(*img_size)\n",
    "fig.supylabel(\"Running time\")\n",
    "for instance, ax in zip(instances,fig.get_axes()):\n",
    "    ax.set_title(instance)\n",
    "    vplot = ax.violinplot(X[instance].dropna(),showextrema=False)\n",
    "    bplot = ax.boxplot(X[instance].dropna(),widths=0.2,medianprops=dict(color=\"black\"),flierprops=flierprops)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True,which=\"major\",ls='-')\n",
    "    ax.grid(True,which=\"minor\",ls='dotted')\n",
    "    ax.yaxis.set_major_locator(mpl.ticker.LogLocator())\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda x, pos: f\"{x/1e6:.2f} ms\"))\n",
    "    ax.set_xticks(np.arange(len(solvers))+1,shorter_solvers[:len(solvers)])\n",
    "    for vp, color in zip(vplot[\"bodies\"],colors):\n",
    "        vp.set_color(color)\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "fig.delaxes(axs[-1, -1])\n",
    "axs[-2,-1].xaxis.set_tick_params(labelbottom=True)\n",
    "img_path = f\"{img}/run_times{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Running times of algorithms]({img_path}){{#fig:run_times}}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in @fig:run_times Steepest is slower than greedy on every tested instance. The running times of random searche and random walk are identical in each repetition. The chosen duration of the algorithms was the average between mean running times of both local searches. As expected the construction heuristic was by far the fastest. Tabu Search and Simulated Annealing run longer, as they were parametrized to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality\n",
    "\n",
    "The quality is measured as a relative gap according to the following formula:\n",
    "\n",
    "$$ Q(v_z) = \\dfrac{v_z-v_o}{v_o} = \\dfrac{v_z}{v_o} - 1$$\n",
    "\n",
    "where $v_z$ is the solution value and $v_o$ are the value of the optimum. In describes the distance to the optimum relative to it. This way it is also better comparable between instances. Lower values are more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Quality of algorithms](img/quality.pdf){#fig:quality}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data.assign(quality=lambda df: quality_to_opt(df.final_value,df.optimal_value)).pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")[\"quality\"]\n",
    "X = X.fillna(X.max())\n",
    "fig, axs = plt.subplots(nrows=4,ncols=3,layout=\"tight\",sharex=True)\n",
    "fig.set_size_inches(*img_size)\n",
    "fig.supylabel(\"Quality\")\n",
    "for instance, ax in zip(instances,fig.get_axes()):\n",
    "    ax.set_title(instance)\n",
    "    vplot = ax.violinplot(X[instance].dropna(),showextrema=False)\n",
    "    bplot = ax.boxplot(X[instance].dropna(),widths=0.2,medianprops=dict(color=\"black\"),flierprops=flierprops)\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "    ax.set_xticks(np.arange(len(solvers))+1,shorter_solvers[:len(solvers)])\n",
    "    for vp, color in zip(vplot[\"bodies\"],colors):\n",
    "        vp.set_color(color)\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "    ax.grid()\n",
    "fig.delaxes(axs[-1, -1])\n",
    "axs[-2,-1].xaxis.set_tick_params(labelbottom=True)\n",
    "img_path = f\"{img}/quality{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Quality of algorithms]({img_path}){{#fig:quality}}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at @fig:quality As expected the random walk and random search usually have the biggest variances when it comes to the values of the solutions. However in some cases the heuristic performed worse than them. Both local searches had very similar performances and similar distributions, although greedy seems to be slightly better. For a few instances, like $\\texttt{esc32g}$ the quality of the heuristic reaches $0\\%$, which means it achieves the optimal value. This could suggest that these instances may be trivial. Overall the best solutions belong to either Tabu Search or Simulated Annealing. In the case of Simulated annealing the variance of the solutions is huge, like in $\\texttt{Tai256c}$, although in cases like $\\texttt{Lipa50a}$ Tabu Search is less stable in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency of algorithms\n",
    "\n",
    "The quality over time is measured as difference of relative gaps between the initial and final solution int relation to the optimum:\n",
    "\n",
    "$$ V(v_i,v_z,t) = \\dfrac{Q(v_i)-Q(v_z)}{t} = \\dfrac{(v_i-v_o)-(v_z-v_o)}{v_ot} = \\dfrac{v_i-v_z}{v_ot}$$\n",
    "\n",
    "where $v_z$ is the solution value and $v_o$ are the value of the optimum. Intuitively it can be though of as 'speed' of the algorithm - how much it improves the initial solution over some time, relative to optima. Higher values are more desirable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Efficiency of algorithms](img/efficiency.pdf){#fig:efficiency}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_raw = data.assign(quality=lambda df: quality_over(df.initial_value,df.final_value,df.optimal_value)).assign(q_over_t=lambda df: df.quality/df.time)\n",
    "X = X_raw.pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")[\"q_over_t\"]\n",
    "fig, axs = plt.subplots(nrows=4,ncols=3,layout=\"tight\",sharex=True,sharey='row')\n",
    "fig.set_size_inches(*img_size)\n",
    "fig.supylabel(\"Quality over time\")\n",
    "for instance, ax in zip(instances,fig.get_axes()):\n",
    "    ax.set_title(instance)\n",
    "    vplot = ax.violinplot(X[instance].dropna(),showextrema=False)\n",
    "    bplot = ax.boxplot(X[instance].dropna(),widths=0.2,medianprops=dict(color=\"black\"),flierprops=flierprops)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True,which=\"major\",ls='-')\n",
    "    ax.grid(True,which=\"minor\",ls='dotted')\n",
    "    ax.yaxis.set_major_locator(mpl.ticker.LogLocator())\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda x, pos: f\"{x*100:.0e} %/ns\"))\n",
    "    ax.set_xticks(np.arange(len(solvers))+1,shorter_solvers[:len(solvers)])\n",
    "    # ax.xaxis.set_major_locator(mpl.ticker.FixedLocator(np.arange(1,len(selected)+1)))\n",
    "    # ax.set_xticklabels(shorter_solvers)\n",
    "    for vp, color in zip(vplot[\"bodies\"],colors):\n",
    "        vp.set_color(color)\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "fig.delaxes(axs[-1, -1])\n",
    "axs[-2,-1].xaxis.set_tick_params(labelbottom=True)\n",
    "img_path = f\"{img}/efficiency{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Efficiency of algorithms]({img_path}){{#fig:efficiency}}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In @fig:efficiency Although greedy and steepest achieve similar qualities, because greedy is much faster, it achieves much better efficiency overall. Actually, in effciency steepest performs similarily to random search and random walk. Overall Tabu Search and Simulated Anneling are less efficient then Local Searches, as they run for much longer, while there is not much quality to improve further.  In some cases the heuristic, when compared to a random initial solution, produces a worse solution. This cannot be exactly visualized on a logarithm scale, because it reaches negative values. It can be seen in @tbl:heur-stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|      |   esc128 |    esc16a |   esc32g |   lipa20a |   lipa50a |   lipa90a |   tai100b |   tai150b |   tai256c |    tai50b |   tai64c |\n",
       "|:-----|---------:|----------:|---------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|---------:|\n",
       "| min  | 3.77e-06 | -1.02e-04 | 1.57e-04 | -4.34e-06 |  5.89e-09 |  4.05e-09 | -4.94e-07 | -1.20e-07 |  1.29e-08 | -1.43e-05 | 3.35e-06 |\n",
       "| mean | 8.53e-06 |  1.54e-04 | 7.36e-04 |  2.45e-06 |  1.74e-07 |  1.79e-08 |  2.39e-07 | -5.02e-08 |  4.31e-08 | -4.26e-06 | 9.70e-06 |\n",
       "| max  | 1.15e-05 |  3.31e-04 | 1.27e-03 |  8.74e-06 |  3.51e-07 |  3.37e-08 |  9.83e-07 |  3.83e-08 |  7.30e-08 |  2.89e-06 | 2.18e-05 |\n",
       "\n",
       ": Heuristic running times {#tbl:heur-stats}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = X_raw[X_raw.solver == \"heuristic\"].pivot(columns=\"instance\",index=\"repetition\")[\"q_over_t\"].describe().loc[[\"min\",\"mean\",\"max\"]]\n",
    "\n",
    "display(Markdown(table.to_markdown(floatfmt=\".2e\")+\"\\n\\n: Heuristic running times {#tbl:heur-stats}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of steps of algorithms.\n",
    "\n",
    "Here the number of steps performed by local searches, tabu search and simulated annealing are compared. Each step is defined as a single move to a neighbour. For steepest the entire neighbourhood is checked and the one giving the best improvement is performed. For greedy the first found improving move is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Number of steps of algorithms](img/steps.pdf){#fig:steps}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected = full_solvers[:2] + full_solvers[-2:]\n",
    "mask_ls = data[\"solver\"].isin(selected)\n",
    "X = data[mask_ls].pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")[\"iterations\"]\n",
    "fig, axs = plt.subplots(nrows=4,ncols=3,layout=\"tight\",sharex=True)\n",
    "fig.set_size_inches(*img_size)\n",
    "fig.supylabel(\"Steps\")\n",
    "for instance, ax in zip(instances,fig.get_axes()):\n",
    "    ax.set_title(instance)\n",
    "    vplot = ax.violinplot(X[instance].dropna(),showextrema=False)\n",
    "    bplot = ax.boxplot(X[instance].dropna(),widths=0.2,medianprops=dict(color=\"black\"))\n",
    "    #ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda x, pos: f\"{x*100:.3e} %/ns\"))\n",
    "    ax.set_xticks(np.arange(len(selected))+1,[solver_to_short_map[slv] for slv in selected])\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True,which=\"major\",ls='-')\n",
    "    ax.grid(True,which=\"minor\",ls='dotted')\n",
    "    ax.yaxis.set_major_locator(mpl.ticker.LogLocator())\n",
    "    for vp, slv in zip(vplot[\"bodies\"],selected):\n",
    "        vp.set_color(color_map[slv])\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "    # ax.grid()\n",
    "fig.delaxes(axs[-1, -1])\n",
    "axs[-2,-1].xaxis.set_tick_params(labelbottom=True)\n",
    "img_path = f\"{img}/steps{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Number of steps of algorithms]({img_path}){{#fig:steps}}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in @fig:steps, as expected greedy peforms more steps of the two local search algorithms, as it does not check the entire neighbourhood. Because of that it does not converge as quickly as steepest. The more advanced metaheuristics do more steps than Local searches as they explore more of the fitness landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of evaluations\n",
    "\n",
    "An evaluation is counted each time an algorithm either evaluates an entire solution or if an algorithm performs a partial evaluation (calculating the delta of a swap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Number of evaluations of algorithms](img/evals.pdf){#fig:evals}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# selected = full_solvers[:-1]\n",
    "# mask_GSRSRW = data[\"solver\"].isin(selected)\n",
    "X = data.pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")[\"evaluations\"]\n",
    "fig, axs = plt.subplots(nrows=4,ncols=3,layout=\"tight\",sharex=True)\n",
    "fig.set_size_inches(8.3,11.7)\n",
    "fig.supylabel(\"Evaluations\")\n",
    "for instance, ax in zip(instances,fig.get_axes()):\n",
    "    ax.set_title(instance)\n",
    "    vplot = ax.violinplot(X[instance].dropna(),showextrema=False)\n",
    "    bplot = ax.boxplot(X[instance].dropna(),widths=0.2,medianprops=dict(color=\"black\"))\n",
    "    #ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda x, pos: f\"{x*100:.3e} %/ns\"))\n",
    "    ax.set_xticks(np.arange(len(shorter_solvers))+1,shorter_solvers)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True,which=\"major\",ls='-')\n",
    "    ax.grid(True,which=\"minor\",ls='dotted')\n",
    "    ax.yaxis.set_major_locator(mpl.ticker.LogLocator())\n",
    "    for vp, color in zip(vplot[\"bodies\"],colors):\n",
    "        vp.set_color(color)\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "    # ax.grid()\n",
    "fig.delaxes(axs[-1, -1])\n",
    "axs[-2,-1].xaxis.set_tick_params(labelbottom=True)\n",
    "img_path = f\"{img}/evals{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Number of evaluations of algorithms]({img_path}){{#fig:evals}}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in @fig:evals that despite steepest performing more steps, it performs less evaluations than steepest. This is exactly because it does not check the entire neighbourhood. It is also expected for random walk to perform more steps than random search. As these are run for the same amount of time and random walk simply peforms much less in a single 'step' compared to random search, it is able to evaluate more solutions. In some cases like $\\texttt{Esc32g}$ the heuristic performs more evaluations than random search. As they run for longer, the metaheuristics evaluate more solutions than local searches on most of the instances. On some howevere like $\\texttt{150b}$ some runs of Simulated Annealing evaluate less solutions than Steepest, which could suggest either premature convergance or quick convergence to an optimal value. Although the first case is much more likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of initial vs final solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependence between the initial quality of solution and final quality for local searches is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Comparison of quality between initial and final solution](img/final_v_initial.pdf){#fig:final-v-initial}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data[mask_ls].assign(\n",
    "    quality_fin=lambda df: quality_to_opt(df.final_value,df.optimal_value)\n",
    "    ).assign(\n",
    "    quality_start=lambda df: quality_to_opt(df.initial_value,df.optimal_value)\n",
    "    ).pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")\n",
    "corr_data = {col:[] for col in [\"instance\",\"solver\",\"correlation\"]}\n",
    "fig, axs = plt.subplots(nrows=4,ncols=3,layout=\"constrained\")\n",
    "fig.set_size_inches(*img_size)\n",
    "fig.supxlabel(\"Initial quality\")\n",
    "fig.supylabel(\"Final quality\")\n",
    "for instance, ax in zip(instances, fig.get_axes()):\n",
    "    ax.set_title(instance)\n",
    "    for solver, color in zip(full_solvers[:2], colors):\n",
    "        x = X[\"quality_start\"][instance][solver]\n",
    "        y = X[\"quality_fin\"][instance][solver]\n",
    "        ax.scatter(x, y, s=3, label=solver_to_short_map[solver],color=color)\n",
    "        corr = np.corrcoef(x,y)[0,1]\n",
    "        corr_data[\"instance\"].append(instance)\n",
    "        corr_data[\"solver\"].append(solver)\n",
    "        corr_data[\"correlation\"].append(corr)\n",
    "        #ax.text(0.9,0.1,f\"{corr:2f}\",transform=ax.transAxes)\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "    #ax.tick_params(axis=\"x\", labelrotation=15)\n",
    "    ax.grid()\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "corr_data = pd.DataFrame.from_dict(corr_data)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "fig.delaxes(axs[-1, -1])\n",
    "img_path = f\"{img}/final_v_initial{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Comparison of quality between initial and final solution]({img_path}){{#fig:final-v-initial}}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally correlation between the final and initial solution value is presented in @tbl:correlations. The exact measure used is the Pearson Correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| solver   |   esc128 |    esc16a |   esc32g |   lipa20a |   lipa50a |   lipa90a |   tai100b |   tai150b |   tai256c |    tai50b |    tai64c |\n",
       "|:---------|---------:|----------:|---------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|\n",
       "| G        | 4.16e-02 | -6.97e-02 | 2.86e-02 | -1.14e-01 | -7.87e-03 |  7.82e-02 |  8.99e-03 | -4.82e-02 | -2.54e-03 | -2.26e-02 | -3.40e-02 |\n",
       "| S        | 5.15e-02 | -4.40e-02 | 5.02e-02 |  2.16e-02 | -7.28e-02 |  1.86e-03 | -6.01e-03 |  7.92e-02 |  3.33e-02 | -9.91e-02 | -1.01e-01 |\n",
       "\n",
       ": Correlation between initial and final solution in Greedy and Steepest for selected instances {#tbl:correlations}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = corr_data.copy()\n",
    "table[\"solver\"] = table[\"solver\"].apply(lambda s: solver_to_short_map[s])\n",
    "table = table.pivot(columns=\"instance\",index=\"solver\")\n",
    "display(Markdown(table[\"correlation\"].to_markdown(floatfmt=\".2e\")+\"\\n\\n: Correlation between initial and final solution in Greedy and Steepest for selected instances {#tbl:correlations}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be expected that better initial solutions would produce better final solutions, by the possibility of being placed in a better region of the solution space. However there is seemingly no significant correlation between the quality of initial solution and the quality of the final solution, at least for the selected instances as presented in @fig:final-v-initial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-random start local search\n",
    "\n",
    "In this section the 300 repetitions of local searches are treated as a single multi-random start local search. It works similarily to random search but each solution in the restart is a local optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Multistart Local search in Greedy and Steepest version](img/multistart.pdf){#fig:multi-start}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data[mask_ls].assign(quality=lambda df: quality_to_opt(df.final_value,df.optimal_value)).pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")[\"quality\"]\n",
    "best = X.cummin()\n",
    "mean = X.expanding().mean()\n",
    "std = X.expanding().std().fillna(0)\n",
    "fig, axs = plt.subplots(nrows=4,ncols=3,layout=\"constrained\",sharex=True)\n",
    "fig.set_size_inches(*img_size)\n",
    "fig.supxlabel(\"Repetitions\")\n",
    "fig.supylabel(\"Quality\")\n",
    "for instance, ax in zip(instances, fig.get_axes()):\n",
    "    ax.set_title(instance)\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "    for solver, color in zip(full_solvers[:2], colors):\n",
    "        u = mean[instance][solver]\n",
    "        ro = std[instance][solver]\n",
    "        ax.plot(best[instance][solver],label=solver_to_short_map[solver]+\" best\",color=color)\n",
    "        ax.plot(u,'--',label=solver_to_short_map[solver]+\" mean\",color=color)\n",
    "        ax.fill_between(np.arange(len(u)),u+ro,(u-ro).clip(0), label=solver_to_short_map[solver]+\" std\",alpha=0.1,color=color)\n",
    "    ax.grid()\n",
    "    ax.set_facecolor(\"whitesmoke\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right',ncols=2)\n",
    "fig.delaxes(axs[-1, -1])\n",
    "axs[-2,-1].xaxis.set_tick_params(labelbottom=True)\n",
    "img_path = f\"{img}/multistart{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Multistart Local search in Greedy and Steepest version]({img_path}){{#fig:multi-start}}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is value to be gained from running the local search algorithm multiple times. This can be seen in @fig:multi-start However the exact number of steps is highly dependent on the instance, as it is an additional parameter. For most of the selected instances it seems $100$ repetitions is enough, as the algorithm usually flatlines. The mean stabilizes and stays roughly the same. Standard deviation is also stable and its value ultimately depends on the instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of local optimas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between instances is measured as the inverse of Hamming distance:\n",
    "\n",
    "$$ {Sim}(x_1, x_2) = 1 - H(x_1,x_2) = 1 - \\dfrac{\\sum_{i=0}^n \\begin{cases} 1 & \\text{if $x_1[i]$ $\\neq$ $x_2[i]$} \\\\ 0 & {otherwise}\\end{cases} }{n}$$\n",
    "\n",
    "This measure seems appropriate for the nature of the problem as the position of a solution $x$ describes the department, while the index at that position describes to which location that department is assigned.\n",
    "\n",
    "The similarities are calculated between all local optima and then averaged, as well as between the local optimum and global optimum.\n",
    "\n",
    "The selected instances are $\\texttt{Lipa90a}$ and $\\texttt{tai100b}$ as the heuristic performs badly on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "mystnb": {
     "figure": {
      "caption": "Similarity of local optimas for instances $\\texttt{lipa20a}$",
      "name": "simils"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Quality of a solution compared with similarity to optimum(left side) or each solution(right side)](img/simils.pdf){#fig:simils}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subset = [\"lipa20a\",\"lipa90a\",\"esc16a\", \"tai100b\"]\n",
    "selected = full_solvers[:2]\n",
    "to = [\"optimal_solution\",\"final_solution\"]\n",
    "to_titles = [\"Wrt optimum\", \"Wrt eachother\"]\n",
    "to_map = {wrt:title for wrt,title in zip(to,to_titles)}\n",
    "X = data[data[\"repetition\"]<100].assign(\n",
    "    quality=lambda df: quality_over(df.initial_value,df.final_value,df.optimal_value)\n",
    "    ).pivot(columns=[\"instance\",\"solver\"], index=\"repetition\")[to+[\"quality\"]]\n",
    "fig, ax = plt.subplots(nrows=4,ncols=2,layout=\"constrained\")\n",
    "fig.set_size_inches(*img_size)\n",
    "for (instance, wrt), ax in zip(product(subset,to),fig.get_axes()):\n",
    "    for solver, color in zip(selected,colors):\n",
    "        y = np.array(list(map(lambda sols: similarity(*sols),product(X[wrt][instance][solver],X[\"final_solution\"][instance][solver]))))\n",
    "        n = np.ceil(np.sqrt(y.shape[0])).astype(int)\n",
    "        y = y.reshape((n,n))\n",
    "        np.fill_diagonal(y,0)\n",
    "        mean_y = (y.mean(0))*(n/(n-1))\n",
    "        x = X[\"quality\"][instance][solver]\n",
    "        ax.scatter(x,mean_y,label=solver_to_short_map[solver],color=color,s=4)\n",
    "        if wrt == to[1]:\n",
    "            np.fill_diagonal(y,mean_y)\n",
    "            std_y = y.std(0)*np.sqrt((n)/(n-1)) \n",
    "            ax.errorbar(x,mean_y,std_y,color=color,linestyle=\"none\",marker=\"none\",alpha=0.1)\n",
    "\n",
    "        ax.xaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "        ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "        ax.set_title(to_map[wrt]+\" in \"+instance)\n",
    "        ax.grid(True)\n",
    "        ax.set_facecolor(\"whitesmoke\")\n",
    "fig.supxlabel(\"Quality\")\n",
    "fig.supylabel(\"Similarity\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='outside lower right',ncols=2)\n",
    "img_path = f\"{img}/simils{img_format}\"\n",
    "plt.savefig(img_path)\n",
    "plt.close()\n",
    "display(Markdown(f\"![Quality of a solution compared with similarity to optimum(left side) or each solution(right side)]({img_path}){{#fig:simils}}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the problem is globally convex, the expectation is that locally optimal solutions are very similar to eachother. However @fig:simils shows that the similarity is low overall, which can stem from how the nature hamming distance and the size of the instances. Simply speaking, high values are not easy to achieve, even on the smaller instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Overall it was quite suprising to see that the greedy version of local search performed better than the steepest, as I have not seen that often in practice. Althought the final quality was similar, the greedy version was much more efficient. In cases of instances like $\\texttt{Esc32g}$ the heuristic was able to find the optimum along with local searches, as well as competing solutions for instances like $\\texttt{Tai256c}$. \n",
    "\n",
    "No significant correlation between the quality of initial solution and final solution in local searches was found. No significant relation between the similarity and the final quality was found, but this could be partly due to how similarity is calculated.\n",
    "\n",
    "Lastly there is value to be gained from running local search multiple times from different starting locations. After around 100 repetitions, which depends on the instance, the algorithm improves after which it stagnates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulties\n",
    "\n",
    "Coming up with an appropriate construction heuristic is not easy as it depends on the problem. In this case its not entirely intuitive on how to do it. The final solution is one that could probably be implemented for most problems, but does not imply good performance. In comparison a heuristic for TSP would add the closest city each time, which in a way is very similar to this solution!\n",
    "\n",
    "There were some difficulties in how to properly pass the specific arguments to the solvers in the program. Finally it was achieved by implementing a ```Experiment class``` which first parses all command lines arguments and then using a ```switch```, launches the appropriate solver with its arguments.\n",
    "\n",
    "Overall implementing the metaheursics correctly and picking proper initial parameters was much harder, than in the case of Local searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduced improvements\n",
    "\n",
    "Initially the heuristic evaluated the whole incomplete solution. The improvement was to just evaluate assignment of the last 'location', which could be seen as analogous to just evaluating a delta of a move in local algorithms. This was it was much faster, as one would expect for a construction heuristic, while not changing the final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
